---
{"dg-publish":true,"permalink":"/coursework/wi-se24-25/ml/math-kernel/","noteIcon":""}
---

---
# Mathematical Derivation of Kernel Methods

## Key Terms and Notation

- **y**: Vector of target values (labels) where yi is the i-th observation
- **w**: Vector of weights for the basis functions
- **x**: Input vector where xi is the i-th input data point
- **Ï†j(x)**: The j-th basis function applied to input x
- **Î¦**: Design matrix where (Î¦)i,j = Ï†j(xi)
- **Î»**: Regularization parameter (controls the trade-off between fit and complexity)
- **N**: Number of data points
- **Mğœ™**: Number of basis functions
- **K**: Kernel matrix where Ki,j = k(xi, xj)
- **v**: Dual weights vector (coefficients in the kernel representation)

## 1. The Regularized Cost Function

### Original Form
The penalized least squares cost function combines the error term and regularization:

```
cost^pen(w) = Î£(yi - Î£wjÏ†j(xi))Â² + Î»Î£wjÂ²
             i=1..N    j=1..Mğœ™      j=1..Mğœ™
```

### Matrix Form
```
cost^pen(w) = (y - Î¦w)áµ€(y - Î¦w) + Î»wáµ€w
```

This form has two components:
1. Data fit term: (y - Î¦w)áµ€(y - Î¦w)
2. Regularization term: Î»wáµ€w

## 2. Finding the Minimum: Implicit Solution

### Take the Derivative
```
âˆ‚cost^pen(w)/âˆ‚w = -2Î¦áµ€(y - Î¦w) + 2Î»w = 0
```

### Solve for w
```
Î¦áµ€(y - Î¦w) = Î»w
Î¦áµ€y - Î¦áµ€Î¦w = Î»w
Î¦áµ€y = (Î¦áµ€Î¦ + Î»I)w
```

### Implicit Solution
```
wpen = (1/Î»)Î¦áµ€(y - Î¦wpen)
```

## 3. The Kernel Transformation

### Key Insight
The solution can be written as a linear combination of transformed input vectors:
```
wpen = Î¦áµ€v = Î£viğœ™(xi)
      i=1..N
```

Where v = (v1, ..., vN)áµ€ are the new coefficients.

### Prediction Function
```
f(x') = ğœ™(x')áµ€wpen 
      = ğœ™(x')áµ€Î¦áµ€v 
      = Î£vik(xi,x')
       i=1..N
```

Where k(xi,x') = ğœ™(xi)áµ€ğœ™(x') is the kernel function.

## 4. Kernel Form of Cost Function

### Substituting the Kernel Representation
```
cost^pen(v) = (y - Î¦Î¦áµ€v)áµ€(y - Î¦Î¦áµ€v) + Î»váµ€Î¦Î¦áµ€v
            = (y - Kv)áµ€(y - Kv) + Î»váµ€Kv
```

Where K = Î¦Î¦áµ€ is the kernel matrix with elements ki,i' = k(xi,xi')

### Explicit Form
```
cost^pen(v) = Î£(yi - Î£vi'k(xi,xi'))Â² + Î»Î£Î£vivi'k(xi,xi')
             i=1    i'=1               i=1 i'=1
```

## 5. Final Solution

### Take Derivative with Respect to v
```
âˆ‚cost^pen(v)/âˆ‚v = 2K(Kv - y) + 2Î»Kv = 0
```

### Solve for v (assuming K is full rank)
```
K(Kv - y) + Î»Kv = 0
K(Kv + Î»v - y) = 0
Kv + Î»v = y
(K + Î»I)v = y
```

### Final Solution
```
vpen = (K + Î»I)â»Â¹y
```

## Important Properties

1. The solution depends only on dot products of basis functions (kernels), not the basis functions themselves.
2. The final predictor is a weighted sum of N kernels.
3. Not all functions expressible as Î£wjÏ†j(x') can be written in kernel form - only those that minimize the cost function for the specific training data.
4. The kernel matrix K must be positive semi-definite.
5. The regularization parameter Î» ensures numerical stability and prevents overfitting.

## Computational Advantage

The kernel method allows us to:
1. Work with potentially infinite-dimensional feature spaces
2. Avoid explicit computation of basis functions
3. Operate in the dual space with N variables instead of Mğœ™ variables
4. Compute predictions using only kernel evaluations

This makes kernel methods particularly powerful for problems where the feature space is very high-dimensional or even infinite-dimensional, while keeping computations tractable.