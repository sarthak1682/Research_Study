---
{"dg-publish":true,"permalink":"/coursework/wi-se24-25/dlai/notes-dlai/","noteIcon":""}
---

---

# 1

![Screenshot 2025-03-31 at 19.37.43.png](/img/user/Attachments/Screenshot%202025-03-31%20at%2019.37.43.png)

# 2

> [!PDF|] [[DLAI_Slides.pdf#page=96&selection=20,1,27,1|DLAI_Slides, p.96]]
> stacking two CNN layers increases the receptive field by ceiling(ğ‘˜âˆ’1/ 2)


> [!PDF|] [[DLAI_Slides.pdf#page=96&selection=31,0,33,1|DLAI_Slides, p.96]]
> pooling layers multiply the receptive field by k


Recurrent Layers
![Attachments/DLAI_Slides.jpg](/img/user/Attachments/DLAI_Slides.jpg)

[[DLAI_Slides.pdf#page=102&rect=27,32,713,387|DLAI_Slides, p.102]]


![Attachments/DLAI_Slides 1.jpg](/img/user/Attachments/DLAI_Slides%201.jpg)

[[DLAI_Slides.pdf#page=103&rect=24,36,651,453|DLAI_Slides, p.103]]

![Attachments/DLAI_Slides 2.jpg](/img/user/Attachments/DLAI_Slides%202.jpg)

[[DLAI_Slides.pdf#page=106&rect=17,180,697,454|DLAI_Slides, p.106]]


> [!PDF|] [[DLAI_Slides.pdf#page=106&selection=4,0,23,60|DLAI_Slides, p.106]]
> forget gate: decides which variables in the memory should be deleted â€¢ input gate: which variables should be updated (ğ‘–_ğ‘¡) and how (C_t^tilda). â€¢ output gate: what of the memory should be propagated to the next state.


![Attachments/DLAI_Slides 3.jpg](/img/user/Attachments/DLAI_Slides%203.jpg)

[[DLAI_Slides.pdf#page=107&rect=23,72,688,519|DLAI_Slides, p.107]]

![Attachments/DLAI_Slides 4.jpg](/img/user/Attachments/DLAI_Slides%204.jpg)

[[DLAI_Slides.pdf#page=110&rect=25,59,694,513|DLAI_Slides, p.110]]


![Attachments/DLAI_Slides 5.jpg](/img/user/Attachments/DLAI_Slides%205.jpg)

[[DLAI_Slides.pdf#page=111&rect=8,-1,723,516|DLAI_Slides, p.111]]![Attachments/DLAI_Slides 6.jpg](/img/user/Attachments/DLAI_Slides%206.jpg)

[[DLAI_Slides.pdf#page=112&rect=14,31,704,516|DLAI_Slides, p.112]]


# 3
problems with Seq2Seq
![Attachments/DLAI_Slides 7.jpg](/img/user/Attachments/DLAI_Slides%207.jpg)

[[DLAI_Slides.pdf#page=118&rect=89,86,610,256|DLAI_Slides, p.118]]


> [!PDF|] [[DLAI_Slides.pdf#page=118&selection=18,0,28,42|DLAI_Slides, p.118]]
> the mechanism to store the most relevant input tokens on the context does not seem strong enough â€¢ a first try to solve this problem are Bi-LSTMs which encode sentence in both directions an concatenate the outputs

![Attachments/DLAI_Slides 8.jpg](/img/user/Attachments/DLAI_Slides%208.jpg)

[[DLAI_Slides.pdf#page=119&rect=29,215,570,513|DLAI_Slides, p.119]]


> [!PDF|] [[DLAI_Slides.pdf#page=119&selection=6,0,26,1|DLAI_Slides, p.119]]
> idea: the context vector for output t of the decoder is a weighted average of Bi-LSTM outputs â€¢ note each output token has its own attention vector ğ›¼ğ‘¡ â€¢ the attention weights measure how well an outputs aligns ğ‘ ğ‘¡ with an encoder hidden state â„

![Attachments/DLAI_Slides 9.jpg](/img/user/Attachments/DLAI_Slides%209.jpg)

[[DLAI_Slides.pdf#page=120&rect=21,64,706,517|DLAI_Slides, p.120]]


![Attachments/DLAI_Slides 10.jpg](/img/user/Attachments/DLAI_Slides%2010.jpg)

[[DLAI_Slides.pdf#page=121&rect=11,78,659,523|DLAI_Slides, p.121]]

> [!PDF|] [[DLAI_Slides.pdf#page=124&selection=0,23,32,19|DLAI_Slides, p.124]]
> Soft and Hard Attention To what part of the input does the output attend? â€¢ soft attention: weighs all inputs [2] â€“ pro: model is smooth and differentiable â€“ con: expensive for large inputs â€¢ hard attention: dynamically restrict attention to a part of the input [3] â€“ pro: requires less calculation at inference time â€“ con: model gets non-differentiable and requires more complicated techniques to train

Instead of looking at all source states, local attention focuses on aÂ subsetÂ or "window" of source hidden states at each target stepÂ t.
At each stepÂ tÂ of the decoder, the global attention mechanism considersÂ allÂ hidden states (h_s) from the encoder (the blue and brown boxes at the bottom).


## NTM

![Attachments/DLAI_Slides 11.jpg](/img/user/Attachments/DLAI_Slides%2011.jpg)

[[DLAI_Slides.pdf#page=126&rect=19,156,356,456|DLAI_Slides, p.126]]

NN + Memory (N x M Matrix)

Reading and Writing 

- **Attention Vector (w_t):**Â This is the crucialÂ attention distributionÂ over theÂ NÂ memory locations at timeÂ t.
    
    - w_t âˆˆ [0, 1]^N: It's a vector of length N, where each elementÂ w_t(i)Â is between 0 and 1.
        
    - ||w|| = 1Â (should likely beÂ Î£ w_t(i) = 1): The elements sum to 1, meaning it acts like a probability distribution or a set of weights indicating how much focus to put on each memory locationÂ i.
        
    - w_t(i)Â is the weight for theÂ i-th row (M_t(i)).
        
- **Reading Vector (r_t):**Â This is how information is readÂ fromÂ memory.
    
    - r_t = Î£_{i=1}^{N} w_t(i) M_t(i): The read vectorÂ r_tÂ is calculated as theÂ weighted averageÂ of all memory rowsÂ M_t(i), using the attention weightsÂ w_t(i). IfÂ w_t(k)Â is high, the content of memory locationÂ kÂ (M_t(k)) will contribute significantly toÂ r_t. ThisÂ r_tÂ is typically passed back to the controller.
        
- **UpdatingÂ M_t:**Â This describes how memory is modified by a write head, going fromÂ M_{t-1}Â toÂ M_t. It's a two-step process applied to each memory locationÂ i, using theÂ sameÂ attention weightsÂ w_tÂ (or a dedicated write attention vector):
    
    - **Erase Step:**
        
        - tilde{M}_t(i) = M_{t-1}(i) [1 - w_t(i) e_t]
            
        - M_{t-1}(i): The content of memory locationÂ iÂ from theÂ previousÂ timestep.
            
        - e_t: An "erase vector" (size M, typically output by the controller). Its elements are usually between 0 and 1.
            
        - w_t(i): The attention weight on locationÂ i.
            
        - The productÂ w_t(i) e_tÂ determines how much of the previous contentÂ M_{t-1}(i)Â is erased. If attentionÂ w_t(i)Â is high and the corresponding element inÂ e_tÂ is 1, that part of the memory vector is multiplied by (1-1)=0, effectively erasing it. If attention is low, orÂ e_tÂ is 0, the content is largely preserved (multiplied by 1).Â tilde{M}_t(i)Â is the intermediate memory state after erasing.
            
    - **Add Step:**
        
        - M_t(i) = tilde{M}_t(i) + w_t(i) a_t
            
        - a_t: An "add vector" (size M, also typically output by the controller). This is the new information to be written.
            
        - The add vectorÂ a_tÂ is weighted by the attentionÂ w_t(i)Â and added to the (potentially partially erased) memory locationÂ tilde{M}_t(i). New information is primarily added to locations where attentionÂ w_t(i)Â is high.
            
- **LearningÂ w_t:**Â States that the critical attention weightsÂ w_tÂ are determined by a combination ofÂ **content-based**Â addressing (focusing based onÂ whatÂ is in memory) andÂ **location-based**Â addressing (focusing based onÂ whereÂ the head was looking previously).


- **Key Vector (k_t):**Â The controller generates a "key" vectorÂ k_tÂ based on the current inputÂ x_tÂ (and its internal state). This key represents theÂ contentÂ the controller is currently interested in finding or writing to in memory.
    
- **Attention (w^c_t(i)Â - Content Weight):**Â This weight reflects how relevant theÂ contentÂ of memory locationÂ iÂ (M_t(i)) is to the keyÂ k_t.
    
    - **Similarity Measure:**Â Cosine similarityÂ cos[k_t, M_t(i)]Â is used to measure the similarity between the key vectorÂ k_tÂ and the content vectorÂ M_t(i)Â stored at locationÂ i. Values range from -1 (opposite) to 1 (identical).
        
    - **Strength (Î²_t):**Â The cosine similarity is multiplied by a positive scalarÂ Î²_tÂ (also output by the controller). This "strength" parameter controls theÂ sharpnessÂ of the attention. A highÂ Î²_tÂ makes the NTM focus very strongly on the memory location(s) most similar to the key, while a lowÂ Î²_tÂ results in a softer, more distributed attention.
        
    - **Softmax:**Â The scaled similarity scores (Î²_t * cos[...]) for all memory locationsÂ iÂ are passed through a softmax function:  
        w^c_t(i) = exp(Î²_t * cos[k_t, M_t(i)]) / Î£_{j=1}^{N} exp(Î²_t * cos[k_t, M_t(j)])  
        This normalizes the scores into a probability distributionÂ w^c_tÂ where weights are positive and sum to 1. LocationsÂ iÂ with contentÂ M_t(i)Â highly similar to the keyÂ k_tÂ will receive large weightsÂ w^c_t(i).
        
- **Interpolation (w^g_tÂ - Gated Weight):**Â Content-based addressing is often combined with information about the previous attention focus.
    
    - **Gate (g_t):**Â The controller outputs an "interpolation gate"Â g_t, a scalar between 0 and 1.
        
    - **Formula:**Â w^g_t = g_t * w^c_t + (1 - g_t) * w_{t-1}
        
    - This calculates a "gated" attentionÂ w^g_tÂ by taking a weighted average of theÂ current content-based attentionÂ w^c_tÂ and theÂ final attention distribution from the previous timestepÂ w_{t-1}.
        
    - IfÂ g_tÂ is close to 1, the NTM primarily uses the newly calculated content-based attention. IfÂ g_tÂ is close to 0, it sticks with the attention distribution from the previous step (w_{t-1}), which might have been influenced by location-based mechanisms. This allows the NTM to either jump to a location based on content or maintain its focus from the previous step.

- **Purpose:**Â While content-based addressing finds locations based onÂ whatÂ they contain, location-based addressing modifies the focus based onÂ whereÂ the head was looking previously. This allows the NTM to learn operations like sequential access ("move to the next location") or focusing on nearby locations.
    
    - "Allows to smoothen the attention weights over closeby rows": It can distribute the focus slightly, preventing it from being overly concentrated on a single location if needed.
        
    - "Like a 1-d convolution of the weights with a fixed kernel s_t": This is the core mechanism. It treats the memory locations as a 1D sequence and applies a convolutional filter (s_t) to the current attention distribution.
        
- **Mechanism: Convolutional Shift**
    
    1. **Input:**Â Takes theÂ gatedÂ attention weightsÂ w^g_tÂ (the output of the interpolation step, which combined content-based weightsÂ w^c_tÂ and previous weightsÂ w_{t-1}).
        
    2. **Shift Kernel (s_t):**Â The controller outputs a small weighting distributionÂ s_t, called the shift kernel or shift weighting. This kernel defines how attention should be shifted. For example, a kernel focused onÂ +1Â would encourage shifting attention one step forward in memory. The slide shows an exampleÂ s_tÂ allowing shifts to positionsÂ -1,Â 0, andÂ +1Â relative to the current focus, with weights {0.2, 0.6, 0.2} respectively. This kernel primarily keeps the focus (0.6Â atÂ 0) but also slightly spreads/shifts it to adjacent locations.
        
    3. **Convolution (tilde{w}_t(i) = Î£_{j=1}^{N} w^g_t(j) s_t(i - j)):**Â A 1D convolution is performed. For each memory locationÂ i, the new unnormalized weightÂ tilde{w}_t(i)Â is calculated by summing contributions from all original locationsÂ j. The contribution from locationÂ jÂ (w^g_t(j)) is weighted by the shift kernel valueÂ s_t(k)Â corresponding to the required shiftÂ k = i - j. (Note: NTMs typically useÂ circularÂ convolution to handle boundaries, meaning locationÂ NÂ is adjacent toÂ 1).
        
    4. **Normalization (w_t(i) = tilde{w}_t(i) / Î£_{j=1}^{N} tilde{w}_t(j)):**Â The resulting weightsÂ tilde{w}_tÂ might not sum to 1 after the convolution. They are re-normalized by dividing each element by the sum of all elements, ensuring the outputÂ w_tÂ (or an intermediate version before sharpening) is a valid probability distribution.
        
- **Examples:**
    
    - The left example shows a typical shift kernelÂ s_tÂ that encourages staying put but allows small shifts left/right.
        
    - The right example text/calculation seems slightly disconnected from the convolution formula shown above it and might be illustrating a different concept or calculation (perhaps related to how an addressÂ p_tÂ could be determined in other attention variants, not standard NTM convolution). The core idea for NTM location-based addressing remains theÂ convolutional shift.
        

**In essence:**Â Location-based addressing uses a learned convolutional kernelÂ s_tÂ to shift and potentially blur the attention distributionÂ w^g_t, allowing the NTM controller to learn relative movements within the memory independent of content.

**Slide 2: Overview over the complete Computation**

This slide provides a flowchart summarizing the entire process of calculating the final attention weightsÂ w_tÂ at a single timestepÂ t, integrating all the components discussed.

- **Inputs:**
    
    - **Previous State:**Â The memoryÂ M_{t-1}Â (implicitly used for content addressing) and the previous timestep's final attention vectorÂ w_{t-1}Â (used for interpolation).
        
    - **Controller Outputs:**Â At each stepÂ t, the controller network processes the current inputÂ x_tÂ and its own internal state to produce several control parameters:
        
        - key vector k_t: Query for content addressing.
            
        - key strength Î²_t: Controls focus sharpness for content addressing.
            
        - interpolation gate g_t: Mixes content vs. previous attention.
            
        - shift weighting s_t: Kernel for the location-based convolutional shift.
            
        - sharpen weighting Î³_t: Final focusing parameter (gamma >= 1).
            
- **Computational Steps (Flowchart):**
    
    1. **Content Addressing:**Â UsesÂ k_t,Â Î²_t, and memoryÂ M_tÂ (orÂ M_{t-1}) to calculate theÂ **content-based**Â attention weightsÂ w^c_tÂ via cosine similarity and softmax. This identifies locations matching the key.
        
    2. **Interpolation:**Â Combines theÂ content-basedÂ weightsÂ w^c_tÂ with theÂ last attention vectorÂ w_{t-1}Â using theÂ interpolation gateÂ g_t. Output is theÂ **gated**Â weight vectorÂ w^g_t = g_t * w^c_t + (1 - g_t) * w_{t-1}. This step decides how much to rely on the new content match versus the previous focus.
        
    3. **Convolutional Shift:**Â Applies theÂ **location-based shifts**. TheÂ gatedÂ weightsÂ w^g_tÂ are convolved with theÂ shift weightingÂ kernelÂ s_tÂ generated by the controller. This performs relative shifts (e.g., move focus left/right/stay put). The output is the shifted (but potentially not final) attention distribution (let's call itÂ w^{conv}_t). (Normalization is implicitly part of this or the next step).
        
    4. **Sharpening:**Â Takes the shifted weightsÂ w^{conv}_tÂ and raises each element to the power of theÂ sharpen weightingÂ Î³_tÂ (gamma), then re-normalizes.Â w_t(i) = (w^{conv}_t(i))^{\gamma_t} / Î£_j (w^{conv}_t(j))^{\gamma_t}. AÂ Î³_t > 1Â makes the distribution peakier (sharper focus), whileÂ Î³_t = 1Â leaves it unchanged. This allows the controller to adjust the precision of the final focus.
        
    5. **Output:**Â The final,Â **sharpened**Â attention weight vectorÂ w_t.
        
- **Usage:**Â This final attention vectorÂ w_tÂ is then used by the read head(s) to compute the read vectorÂ r_tÂ (asÂ Î£ w_t(i) M_t(i)) and by the write head(s) to perform the erase/add operations (usingÂ w_t(i) e_tÂ andÂ w_t(i) a_t).


## Pointer Nets
- **Core Task:**Â Pointer Networks are designed for problems where the goal is toÂ **compute an order or selection over elements from the input sequence**. The output isn't generatingÂ newÂ words or values, but ratherÂ pointingÂ to specific elements within the input.
    
- **Example: Traveling Salesman Problem (TSP):**
    
    - **Input:**Â A sequence of cities (e.g., represented by their 2D coordinates).
        
    - **Output/Solution:**Â A visiting order (a permutation) of the input cities that minimizes the total travel distance. This order is a sequence ofÂ pointersÂ back to the original input cities.
        
- **Key Idea:**Â The core mechanism is to useÂ **attention**, but in a novel way. Standard attention mechanisms compute alignment scores, normalize them (softmax) to get weights, and then use these weights to calculate a weighted average (context vector) of the input states. Pointer NetworksÂ **skip the weighted average step**. They use the normalized attention scoresÂ directlyÂ as a probability distribution over the input elements. At each step, the model outputs theÂ indexÂ of the input element that receives the highest attention score.
    
- **Diagrams:**
    
    - The left diagram shows a simple TSP instance with 4 points (cities).
        
    - The right diagram sketches the architecture. The bottom part is anÂ **encoder**Â (likely an RNN like LSTM) processing the input sequence (x1Â toÂ x4). The top part is aÂ **decoder**Â (also likely an RNN). At each decoding stepÂ t, the decoder uses its current state and the encoder hidden states to compute attention scores over the inputs. The arrows pointing back down indicate that the output for that step isÂ selectingÂ one of the input positions (e.g., step 1 selects input 2, step 2 selects input 4, etc.).
        
![Attachments/DLAI_Slides 12.jpg](/img/user/Attachments/DLAI_Slides%2012.jpg)

[[DLAI_Slides.pdf#page=131&rect=26,40,642,312|DLAI_Slides, p.131]]
**In essence:**Â Pointer Networks address the limitation of standard seq2seq models where the output vocabulary size is fixed. Here, the "vocabulary" for each output step is the set of input elements for thatÂ specificÂ input sequence, making it suitable for variable-length input/output mapping problems like sorting, TSP, convex hull finding, etc.

- **Input/Output Formalized:**
    
    - **Input:**Â A sequence ofÂ nÂ vectorsÂ X = x_1, ..., x_nÂ (e.g., coordinates, word embeddings).
        
    - **Output:**Â A sequence ofÂ m(X)Â indicesÂ C = c_1, ..., c_{m(X)}. EachÂ c_tÂ is an integer between 1 andÂ n, indicating which input element is selected at stepÂ t. The output lengthÂ m(X)Â might depend on the input (e.g.,Â nÂ for TSP or sorting).
        
- **Architecture:**Â Based on a standardÂ **sequential encoder-decoder**Â framework (e.g., using LSTMs or GRUs).
    
    - The encoder processes the inputÂ XÂ and generates a sequence of hidden statesÂ h_1, ..., h_n.
        
    - The decoder generates the output sequence of indicesÂ c_1, ..., c_{m(X)}Â one step at a time, maintaining its own sequence of hidden statesÂ s_1, ..., s_{m(X)}.
        
- **Attention Vector Calculation (The "Pointing" Mechanism):**
    
    - At each decoder time stepÂ t:
        
        1. **Calculate Alignment Scores (u^t):**Â Use an attention scoring function (here, additive attention is shown) to compute a scoreÂ u^t_jÂ for each input positionÂ jÂ (from 1 toÂ n). This score measures the relevance of inputÂ jÂ (represented by encoder stateÂ h_j) given the current decoder stateÂ s_t.
            
            - u^t_j = v^T tanh(W_1 h_j + W_2 s_t)Â (This is a common way to calculate the score).Â v,Â W_1,Â W_2Â are learnable parameters.
                
        2. **Calculate Probability Distribution (p):**Â Apply theÂ **softmax**Â functionÂ directlyÂ to the vector of alignment scoresÂ u^t = [u^t_1, ..., u^t_n].
            
            - p(c_t | c_1, ..., c_{t-1}, X, Î¸) = softmax(u^t)
                
            - This gives a probability distribution over theÂ input indicesÂ {1, ..., n}. The valueÂ p(c_t = j | ...)Â is the model's predicted probability that the correct output at stepÂ tÂ is the indexÂ j.
                
- **Training Objective:**
    
    - The model is trained usingÂ **maximum likelihood**. The goal is to find the parametersÂ Î¸Â that maximize the log probability of the true output sequencesÂ CÂ given the input sequencesÂ XÂ in the training data.
        
    - The probability of the entire sequenceÂ p(C|X, Î¸)Â is calculated using the chain rule: multiply the probabilities of generating each indexÂ c_tÂ conditioned on the inputÂ XÂ and the previously generated indicesÂ c_1, ..., c_{t-1}. Each conditional probabilityÂ p(c_t | ...)Â is given by the softmax output described above
---


[[Coursework/WiSe24-25/dl4nlp/Notes/W2_dl4nlp#BPE\|W2_dl4nlp#BPE]]

[[Coursework/WiSe24-25/dl4nlp/Notes/BLEU\|BLEU]]


[[Coursework/WiSe24-25/dl4nlp/Notes/W3_dl4nlp_Transformers\|W3_dl4nlp_Transformers]]

[[Coursework/SoSe24/GenAI/Ch 6 ARMs#ViT\|Ch 6 ARMs#ViT]]


[[Coursework/SoSe24/GenAI/Ch 3 Netw. Vis & Understanding#U-Net\|Ch 3 Netw. Vis & Understanding#U-Net]]
