---
{"dg-publish":true,"permalink":"/coursework/wi-se24-25/theo-phil-ai/paper/paper-res/alignment-course/","noteIcon":""}
---

----
**1. Outer alignment: Specify goals to an AI system correctly.  
_Also known as solving: reward misspecification, reward hacking, specification gaming, Goodharting


**2. Inner alignment: Get AI to follow these goals.  
_Also known as solving: goal misgeneralization



OG Thesis

![Screenshot 2024-10-31 at 11.20.49.png](/img/user/Attachments/Screenshot%202024-10-31%20at%2011.20.49.png)




![Screenshot 2024-10-31 at 11.44.46.png](/img/user/Attachments/Screenshot%202024-10-31%20at%2011.44.46.png)

---

From [Deep Forgetting & Unlearning for Safely-Scoped LLMs — AI Alignment Forum](https://www.alignmentforum.org/posts/mFAvspg4sXkrfZ7FA/deep-forgetting-and-unlearning-for-safely-scoped-llms)

Interesting Stuff on Scoping

[\[2304.11082\] Fundamental Limitations of Alignment in Large Language Models](https://arxiv.org/abs/2304.11082)
 provide a theoretical argument as to why jailbreaks might be a persistent problem for LLMs.

---
