---
{"dg-publish":true,"permalink":"/zotero-storage/9-qk-5-h5-zg/boden-notes-ch-1-4/","noteIcon":""}
---

----

# Ch1


> [!PDF|187, 97, 229] [[Boden - 2016 - AI its nature and future.pdf#page=15&annotation=2191R|Boden - 2016 - AI its nature and future, p.4]]
> > a neural network (see Chapter 4) is thought of as doing information processing in parallel, even though it’s usually implemented in a (sequential) von Neumann computer

Refresh: [[Von Neumann Machines\|Von Neumann Machines]]

> [!PDF|187, 97, 229] [[Boden - 2016 - AI its nature and future.pdf#page=17&annotation=2194R|Boden - 2016 - AI its nature and future, p.6]]
> >  five major types, each including many variations. One is classical, or symbolic, AI—sometimes called GOFAI (Good OldFashioned AI). Another is artificial neural networks, or connectionism. In addition, there are evolutionary programming; cellular automata; and dynamical systems.

An Intro to each of the Five Major Types: 
[[GOFAI\|GOFAI]]
[[Connectionism\|Connectionism]]
[[Evolutionary Programming\|Evolutionary Programming]]
[[CA\|CA]]
[[Dynamical Systems View of AI\|Dynamical Systems View of AI]]

> [!PDF|187, 97, 229] [[Boden - 2016 - AI its nature and future.pdf#page=20&annotation=2197R|Boden - 2016 - AI its nature and future, p.9]]
> >  In their paper “A Logical Calculus of the Ideas Immanent in Nervous Activity,”4 they united Turing’s work with two other exciting items (both dating from the early twentieth century): Bertrand Russell’s propositional logic and Charles Sherrington’s theory of neural synapses
> 

At least summarise the Ideas from all 3 
[[Logical Calc..\|Logical Calc..]] by Walter and Pitts
[[Russell Prop_Logic\|Russell Prop_Logic]] by Russell


> [!PDF|187, 97, 229] [[Boden - 2016 - AI its nature and future.pdf#page=27&annotation=2200R|Boden - 2016 - AI its nature and future, p.16]]
> > This GOFAI program learned to recognize patterns by having many bottomlevel “demons,” each always looking out for one simple perceptual input, which passed their results on to higher level demons.
> 

Refresh: [[Daemon\|Daemon]]


# Ch4 ANNs

Four Advantages: 
1) association between patterns without being explicitly programmed
2) tolerance of "messy" evidence
3) recognise incomplete/partly damaged patterns (Content Addr. Memory)
4) Robust (ie showing graceful degradation)

[[Past-Tense Learner\|Past-Tense Learner]] : What are the implications? 

> [!PDF|187, 97, 229] [[Boden - 2016 - AI its nature and future.pdf#page=95&annotation=2203R|Boden - 2016 - AI its nature and future, p.84]]
> > PDP devotees argue, for example, that this approach refutes the Physical Symbol System hypothesis
> 

What's the hypothesis [[Physical Symbol System Hypothesis\|Physical Symbol System Hypothesis]]

> [!PDF|187, 97, 229] [[Boden - 2016 - AI its nature and future.pdf#page=97&annotation=2206R|Boden - 2016 - AI its nature and future, p.86]]
> > Given that a PDP network is using some Hebbian learning rule to adapt its weights, when does it stop? The answer isn’t When it has achieved perfection (all inconsistencies eliminated), but When it has achieved maximum coherence

Ok!

Formulation of the Credit-Assignment Problem: [[Credit-Assignment\|Credit-Assignment]]
[[Helmholtz Machines\|Helmholtz Machines]]
[[RBMs\|RBMs]]
[[Hopfield Nets\|Hopfield Nets]]

[[Predictive coding\|Predictive coding]] (A Model of Sensorimotor Control)

> [!PDF|187, 97, 229] [[Boden - 2016 - AI its nature and future.pdf#page=102&annotation=2209R|Boden - 2016 - AI its nature and future, p.91]]
> > much as the DQN algorithm simulates processes in visual cortex and hippocampus

[[DQN\|DQN]] (Also, how does it mirror those processes, also, what are those processes!)

> [!PDF|187, 97, 229] [[Boden - 2016 - AI its nature and future.pdf#page=103&annotation=2212R|Boden - 2016 - AI its nature and future, p.92]]
> > Rosenblatt’s self-organizing perceptrons

How were they "Self-Organizing"

> [!PDF|187, 97, 229] [[Boden - 2016 - AI its nature and future.pdf#page=111&annotation=2215R|Boden - 2016 - AI its nature and future, p.100]]
> > In short, the virtual machines implemented in our brains are both sequential and parallel. Human intelligence requires subtle cooperation between them. And human-level AGI, if it’s ever achieved, will do so too.

On Neurosymbolic AI 



----
Pytorch directly uses Logits? What are the possible Advantages> 
