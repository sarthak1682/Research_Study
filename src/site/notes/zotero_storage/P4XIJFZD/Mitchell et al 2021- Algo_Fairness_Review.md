---
{"dg-publish":true,"permalink":"/zotero-storage/p4-xijfzd/mitchell-et-al-2021-algo-fairness-review/","noteIcon":""}
---

---
> [!PDF|255, 208, 0] [[Mitchell et al. - 2021 - Algorithmic Fairness Choices, Assumptions, and De.pdf#page=4&annotation=758R|Mitchell et al. - 2021 - Algorithmic Fairness Choices, Assumptions, and De, p.144]]
>  data with these undesirable properties are often labeled informally as biased



> [!PDF|255, 208, 0] [[Mitchell et al. - 2021 - Algorithmic Fairness Choices, Assumptions, and De.pdf#page=5&annotation=761R|Mitchell et al. - 2021 - Algorithmic Fairness Choices, Assumptions, and De, p.145]]
>  shift in covariates

#Q 

> [!PDF|255, 208, 0] [[Mitchell et al. - 2021 - Algorithmic Fairness Choices, Assumptions, and De.pdf#page=9&annotation=764R|Mitchell et al. - 2021 - Algorithmic Fairness Choices, Assumptions, and De, p.149]]
>  Here, a decision is considered to be fair if individuals with the same score s i = ψ(vi) are treated equally, regardless of group membershi

> [!PDF|255, 208, 0] [[Mitchell et al. - 2021 - Algorithmic Fairness Choices, Assumptions, and De.pdf#page=12&annotation=777R|Mitchell et al. - 2021 - Algorithmic Fairness Choices, Assumptions, and De, p.152]]
> We now turn to fairness notions that focus on decisions  D  without consideration of  Y. These can be motivated in a few ways. Suppose that, from the perspective of the individuals about whom we make decisions, one decision is always preferable to another, regardless of  Y  (e.g., nondetention)

this sounds so utterly idiotic, you are not making a decision just about the detained but also for the rest of the society. lol, wtf. 


> [!PDF|255, 208, 0] [[Mitchell et al. - 2021 - Algorithmic Fairness Choices, Assumptions, and De.pdf#page=13&annotation=780R|Mitchell et al. - 2021 - Algorithmic Fairness Choices, Assumptions, and De, p.153]]
> In 2016, ProPublica published a highly influential analysis based on data obtained through public records requests (Angwin et al. 2016). Their most discussed finding was that COMPAS does not satisfy equal false positive rates by race: Among defendants who did not get rearrested, black defendants were twice as likely to be misclassified as high risk. Based largely on this and other similar findings, they described the tool as biased against blacks. Northpointe (now Equivant), the developers of COMPAS, critiqued ProPublica’s work and pointed out that COMPAS satisfies equal positive predictive values: among those called higher risk, the proportion of defendants who got rearrested is approximately the same regardless of race (Dieterich et al. 2016). COMPAS also satisfies calibration within groups (Flores et al. 2016). Much of the subsequent conversation consisted of either trying to harmonize these definitions of fairness or asserting that one or the other is correct. As it turns out, there can be no harmony among definitions in a world where inequality and imperfect prediction are the reality.

follow this, also, what were the results of being labeled "high risk"


> [!PDF|255, 208, 0] [[Mitchell et al. - 2021 - Algorithmic Fairness Choices, Assumptions, and De.pdf#page=15&annotation=783R|Mitchell et al. - 2021 - Algorithmic Fairness Choices, Assumptions, and De, p.155]]
> hat we can define a metric in the observed space that approximates a metric in the construct space

shouldn't it be the opposite? 

> [!PDF|255, 208, 0] [[Mitchell et al. - 2021 - Algorithmic Fairness Choices, Assumptions, and De.pdf#page=13&annotation=780R|Mitchell et al. - 2021 - Algorithmic Fairness Choices, Assumptions, and De, p.153]]
> In 2016, ProPublica published a highly influential analysis based on data obtained through public records requests (Angwin et al. 2016). Their most discussed finding was that COMPAS does not satisfy equal false positive rates by race: Among defendants who did not get rearrested, black defendants were twice as likely to be misclassified as high risk. Based largely on this and other similar findings, they described the tool as biased against blacks. Northpointe (now Equivant), the developers of COMPAS, critiqued ProPublica’s work and pointed out that COMPAS satisfies equal positive predictive values: among those called higher risk, the proportion of defendants who got rearrested is approximately the same regardless of race (Dieterich et al. 2016). COMPAS also satisfies calibration within groups (Flores et al. 2016). Much of the subsequent conversation consisted of either trying to harmonize these definitions of fairness or asserting that one or the other is correct. As it turns out, there can be no harmony among definitions in a world where inequality and imperfect prediction are the reality.
