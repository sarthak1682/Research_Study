---
{"dg-publish":true,"permalink":"/zotero-storage/g76-ep-8-gc/consciousness-ai-23/","noteIcon":""}
---

---

> [!PDF|255, 208, 0] [[Butlin et al. - 2023 - Consciousness in Artificial Intelligence Insights.pdf#page=4&annotation=960R|Butlin et al. - 2023 - Consciousness in Artificial Intelligence Insights, p.4]]
> computational functionalism, the thesis that performing computations of the right kind is necessary and sufficient for consciousness, as a working hypothesis.


> [!PDF|255, 208, 0] [[Butlin et al. - 2023 - Consciousness in Artificial Intelligence Insights.pdf#page=10&annotation=963R|Butlin et al. - 2023 - Consciousness in Artificial Intelligence Insights, p.10]]
> 2003). In using the term “phenomenal consciousness”, we mean to distinguish our topic from “access consciousness

yes, wtaf? 

and for being the greatest minds, these people do no seem to understand "subjective experience", as if there is "objective experience", shame!


> [!PDF|255, 208, 0] [[Butlin et al. - 2023 - Consciousness in Artificial Intelligence Insights.pdf#page=23&annotation=971R|Butlin et al. - 2023 - Consciousness in Artificial Intelligence Insights, p.23]]
> However, it can also be interpreted as a theory of phenomenal consciousness, motivated by the thought that access consciousness and phenomenal consciousness may coincide, or even be the same property, despite being conceptually distinct

finally some sense!

> [!PDF|255, 208, 0] [[Butlin et al. - 2023 - Consciousness in Artificial Intelligence Insights.pdf#page=58&annotation=982R|Butlin et al. - 2023 - Consciousness in Artificial Intelligence Insights, p.58]]
> An interpretation of Transformers by Elhage et al. (2021) describes them as made up of “residual blocks”, each consisting of one layer of each type, which process information drawn from and then added back to a “residual stream”.


This is important #imp 

> [!PDF|255, 208, 0] [[Butlin et al. - 2023 - Consciousness in Artificial Intelligence Insights.pdf#page=60&annotation=985R|Butlin et al. - 2023 - Consciousness in Artificial Intelligence Insights, p.60]]
> rent task. As in the case of Transformers, the clearest missing element of the global workspace in the Perceiver is the lack of global broadcast. Perceiver IO has multiple output modules, but on any given trial, its inputs include an “output query”, which specifies what kind of output is required. So only one output module acts on information from the workspace. Furthermore, input modules do not generally receive information from the workspace.
